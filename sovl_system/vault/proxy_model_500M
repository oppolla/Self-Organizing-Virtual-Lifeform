import torch
import torch.nn as nn
import math

class ProxyDeepSeek500M(nn.Module):
    """A 500M parameter proxy model mimicking a DeepSeek-like transformer LLM."""
    def __init__(
        self,
        vocab_size=50240,  # Adjusted for ~500M with tying
        hidden_size=1280,  # Tuned for exact 500M
        num_layers=16,
        num_heads=16,
        ffn_hidden_size=5120,  # 4 * hidden_size
        max_length=1024,
        dropout=0.1,
        use_float16=False,
        tie_weights=True,  # Default for efficiency
        enable_cyclic_test=False
    ):
        super(ProxyDeepSeek500M, self).__init__()
        self.hidden_size = hidden_size
        self.max_length = max_length
        self.enable_cyclic_test = enable_cyclic_test
        self.cyclic_layer_ref = None

        # Token embedding
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        
        # Transformer layers (pre-LN)
        self.transformer_layers = nn.ModuleList([
            nn.ModuleDict({
                'attn_norm': nn.LayerNorm(hidden_size),
                'attention': nn.MultiheadAttention(
                    embed_dim=hidden_size,
                    num_heads=num_heads,
                    dropout=dropout,
                    batch_first=True
                ),
                'ffn_norm': nn.LayerNorm(hidden_size),
                'ffn': nn.Sequential(
                    nn.Linear(hidden_size, ffn_hidden_size),
                    nn.GELU(approximate='none'),
                    nn.Linear(ffn_hidden_size, hidden_size),
                    nn.Dropout(dropout)
                )
            }) for _ in range(num_layers)
        ])
        
        # Output layer (tied by default)
        self.output_layer = nn.Linear(hidden_size, vocab_size) if not tie_weights else None
        self.tie_weights = tie_weights
        
        # Initialize weights
        self.init_weights()
        
        # Convert to float16 if specified
        if use_float16:
            self.half()

        # Precompute causal mask
        self.register_buffer('causal_mask', torch.triu(
            torch.full((max_length, max_length), float('-inf')), diagonal=1
        ))

    def init_weights(self):
        """Initialize weights with Xavier and normal distributions."""
        for name, param in self.named_parameters():
            if 'weight' in name and param.dim() > 1 and (not self.tie_weights or 'embedding' in name):
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.normal_(param, mean=0.0, std=0.02)

    def _apply_rotary_pos_emb(self, x, position_ids):
        """Apply rotary positional embeddings (RoPE)."""
        def rotary_matrix(pos, dim):
            theta = 10000 ** (-2 * torch.arange(0, dim, 2, device=pos.device) / dim)
            angles = pos.unsqueeze(-1) * theta
            return torch.stack([torch.cos(angles), -torch.sin(angles), torch.sin(angles), torch.cos(angles)], dim=-1)

        seq_len = x.size(1)
        rotary_dim = self.hidden_size // self.num_heads
        rotary = rotary_matrix(position_ids, rotary_dim).view(1, seq_len, 1, rotary_dim // 2, 4)
        
        x_reshaped = x.view(x.size(0), seq_len, self.num_heads, -1)
        q, k = x_reshaped[..., :rotary_dim], x_reshaped[..., :rotary_dim]
        
        q_rot = torch.cat([
            q[..., ::2] * rotary[..., 0] + q[..., 1::2] * rotary[..., 1],
            q[..., ::2] * rotary[..., 2] + q[..., 1::2] * rotary[..., 3]
        ], dim=-1)
        k_rot = torch.cat([
            k[..., ::2] * rotary[..., 0] + k[..., 1::2] * rotary[..., 1],
            k[..., ::2] * rotary[..., 2] + k[..., 1::2] * rotary[..., 3]
        ], dim=-1)
        
        x = x.clone()
        x[..., :rotary_dim] = torch.cat([q_rot, k_rot], dim=-1).view_as(x[..., :rotary_dim])
        return x

    @torch.compile(fullgraph=True, dynamic=True)  # Optimize with torch.compile
    def forward(self, input_ids, attention_mask=None):
        """
        Forward pass for the proxy model.
        Args:
            input_ids: Tensor of shape [batch_size, seq_length]
            attention_mask: Optional padding mask [batch_size, seq_length]
        Returns:
            logits: Tensor of shape [batch_size, seq_length, vocab_size]
        """
        batch_size, seq_length = input_ids.shape
        device = input_ids.device
        
        # Embed tokens
        x = self.embedding(input_ids)
        
        # Position IDs for RoPE
        position_ids = torch.arange(seq_length, device=device).unsqueeze(0).expand(batch_size, -1)
        
        # Apply padding mask
        if attention_mask is not None:
            attention_mask = attention_mask.unsqueeze(-1).unsqueeze(-2)  # [batch, 1, seq_len, 1]
        
        # Slice causal mask
        causal_mask = self.causal_mask[:seq_length, :seq_length]
        
        # Simulate cyclic dependency
        if self.enable_cyclic_test and self.cyclic_layer_ref is None:
            self.cyclic_layer_ref = self.transformer_layers[-1]  # Circular ref to last layer
        
        # Transformer layers
        for i, layer in enumerate(self.transformer_layers):
            # Pre-LN
            x_norm = layer['attn_norm'](x)
            
            # Apply RoPE to queries and keys (simplified)
            x_norm = self._apply_rotary_pos_emb(x_norm, position_ids)
            
            # Attention
            attn_output, _ = layer['attention'](
                x_norm, x_norm, x_norm,
                attn_mask=causal_mask,
                key_padding_mask=(1 - attention_mask.squeeze(-1)).bool() if attention_mask is not None else None,
                need_weights=False
            )
            x = x + attn_output
            
            # FFN
            x_norm = layer['ffn_norm'](x)
            ffn_output = layer['ffn'](x_norm)
            x = x + ffn_output
            
            # Simulate cyclic call to last layer
            if self.enable_cyclic_test and i == len(self.transformer_layers) - 2:
                x = self.cyclic_layer_ref['ffn'](self.cyclic_layer_ref['ffn_norm'](x))
        
        # Output logits
        logits = x @ self.embedding.weight.t() if self.tie_weights else self.output_layer(x)
        self.cyclic_layer_ref = None  # Reset cyclic reference
        return logits

def count_parameters(model):
    """Count total parameters in the model."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def test_model(model):
    """Test model with dummy inputs."""
    model.eval()
    input_ids = torch.randint(0, model.embedding.num_embeddings, (2, 128))
    attention_mask = torch.ones(2, 128)
    attention_mask[1, 100:] = 0  # Simulate padding
    with torch.no_grad():
        output = model(input_ids, attention_mask)
    print(f"Output shape: {output.shape}")
    assert output.shape == (2, 128, model.embedding.num_embeddings), "Unexpected output shape"
    print("Test passed")

# Instantiate and verify
if __name__ == "__main__":
    model = ProxyDeepSeek500M(use_float16=torch.cuda.is_available())
    print(f"Total parameters: {count_parameters(model):,}")
    test_model(model)
    
    # Test cyclic mode
    cyclic_model = ProxyDeepSeek500M(enable_cyclic_test=True)
    try:
        cyclic_model(torch.randint(0, 50240, (1, 32)))
    except Exception as e:
        print(f"Caught expected error: {e}")